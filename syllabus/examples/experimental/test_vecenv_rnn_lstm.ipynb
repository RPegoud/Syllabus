{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4300/4162829367.py:6: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  from distutils.util import strtobool\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import importlib\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(6, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = layer_init(\n",
    "            nn.Linear(512, envs.unwrapped.single_action_space.n), std=0.01\n",
    "        )\n",
    "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        x = x.clone()\n",
    "        x[:, :, :, [0, 1, 2, 3]] /= 255.0\n",
    "        return self.critic(self.network(x.permute((0, 3, 1, 2))))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        x = x.clone()\n",
    "        x[:, :, :, [0, 1, 2, 3]] /= 255.0\n",
    "        hidden = self.network(x.permute((0, 3, 1, 2)))\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    seed: int = 0\n",
    "    torch_deterministic: bool = True\n",
    "    num_envs: int = 16\n",
    "    env_id: str = \"pong_v3\"\n",
    "    batch_size: int = 32\n",
    "    rollout_length: int = 128\n",
    "    learning_rate: float = 1e-5\n",
    "    num_minibatches: int = 4\n",
    "    total_timesteps: int = 1e4\n",
    "    anneal_lr: int = True\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_coef: float = 0.1\n",
    "    clip_vloss: float = True\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    target_kl: float = None\n",
    "    max_grad_norm: float = 0.5\n",
    "    update_epochs: int = 4\n",
    "    norm_adv: bool = True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    args.batch_size = int(args.num_envs * args.rollout_length)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    env = importlib.import_module(f\"pettingzoo.atari.{args.env_id}\").parallel_env()\n",
    "    env = ss.max_observation_v0(env, 2)\n",
    "    env = ss.frame_skip_v0(env, 4)\n",
    "    env = ss.clip_reward_v0(env, lower_bound=-1, upper_bound=1)\n",
    "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "    env = ss.frame_stack_v1(env, 4)\n",
    "    env = ss.agent_indicator_v0(env, type_only=False)\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    envs = ss.concat_vec_envs_v1(\n",
    "        env, args.num_envs // 2, num_cpus=0, base_class=\"gymnasium\"\n",
    "    )\n",
    "    envs.single_observation_space = envs.observation_space\n",
    "    envs.unwrapped.single_action_space = envs.action_space\n",
    "    envs.is_vector_env = True\n",
    "    # envs = gymnasium.wrappers.RecordEpisodeStatistics(envs)\n",
    "\n",
    "    assert isinstance(\n",
    "        envs.unwrapped.single_action_space, gymnasium.spaces.discrete.Discrete\n",
    "    ), \"only discrete action space is supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros(\n",
    "    (args.rollout_length, args.num_envs) + envs.single_observation_space.shape\n",
    ").to(device)\n",
    "actions = torch.zeros(\n",
    "    (args.rollout_length, args.num_envs) + envs.single_action_space.shape\n",
    ").to(device)\n",
    "logprobs = torch.zeros((args.rollout_length, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.rollout_length, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.rollout_length, args.num_envs)).to(device)\n",
    "values = torch.zeros((args.rollout_length, args.num_envs)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)\n",
    "num_updates = int(args.total_timesteps // args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbatchify(x):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    possible_agents = [f\"agent_{i}\" for i in range(len(x))]\n",
    "    x = {idx: x[idx] for idx, agent in enumerate(possible_agents)}\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610f055da9cb4fd69e48906c370c5301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(total=num_updates) as pbar:\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.rollout_length):\n",
    "            global_step += 1 * args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, done, trunc, info = envs.step(action.cpu().numpy())\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(\n",
    "                done\n",
    "            ).to(device)\n",
    "\n",
    "            for idx, item in enumerate(info):\n",
    "                player_idx = idx % 2\n",
    "                if \"episode\" in item.keys():\n",
    "                    print(\n",
    "                        f\"global_step={global_step}, {player_idx}-episodic_return={item['episode']['r']}\"\n",
    "                    )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.rollout_length)):\n",
    "                if t == args.rollout_length - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = (\n",
    "                    rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                )\n",
    "                advantages[t] = lastgaelam = (\n",
    "                    delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                )\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n",
    "                    b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [\n",
    "                        ((ratio - 1.0).abs() > args.clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (\n",
    "                        mb_advantages.std() + 1e-8\n",
    "                    )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(\n",
    "                    ratio, 1 - args.clip_coef, 1 + args.clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None:\n",
    "                if approx_kl > args.target_kl:\n",
    "                    break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        pbar.update(1)\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
