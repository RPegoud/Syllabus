{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, Tuple, TypeVar, Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from lasertag_dr import batchify, unbatchify, Agent\n",
    "\n",
    "from syllabus.core import TaskWrapper\n",
    "\n",
    "sys.path.append(\"../../..\")\n",
    "from lasertag import (  # noqa\n",
    "    LasertagArena1,\n",
    "    LasertagArena2,\n",
    "    LasertagCorridor1,\n",
    "    LasertagCorridor2,\n",
    "    LasertagCross,\n",
    "    LasertagLargeCorridor,\n",
    "    LasertagMaze1,\n",
    "    LasertagMaze2,\n",
    "    LasertagRuins,\n",
    "    LasertagRuins2,\n",
    "    LasertagSixteenRoomsN2,\n",
    "    LasertagStar,\n",
    ")\n",
    "\n",
    "AgentType = TypeVar(\"AgentType\")\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "AgentCurriculum = TypeVar(\"AgentCurriculum\")\n",
    "EnvCurriculum = TypeVar(\"EnvCurriculum\")\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "ObsType = TypeVar(\"ObsType\")\n",
    "\n",
    "\n",
    "class LasertagFixedParallelWrapper(TaskWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper ensuring compatibility with the PettingZoo Parallel API.\n",
    "    Used with fixed Lasertag environments (deterministic resets for benchmarking).\n",
    "\n",
    "    Lasertag Environment:\n",
    "        * Action shape:  `n_agents` * `Discrete(5)`\n",
    "        * Observation shape: Dict('image': Box(0, 255, (`n_agents`, 3, 5, 5), uint8))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_agents = n_agents\n",
    "        self.task = None\n",
    "        self.episode_return = 0\n",
    "        self.possible_agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "        self.n_steps = 0\n",
    "        self.max_steps = self.env.max_steps\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Delegate attribute lookup to the wrapped environment if the attribute\n",
    "        is not found in the LasertagParallelWrapper instance.\n",
    "        \"\"\"\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def _np_array_to_pz_dict(self, array: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing individual observations for each agent.\n",
    "        Assumes that the batch dimension represents individual agents.\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        for idx, value in enumerate(array):\n",
    "            out[self.possible_agents[idx]] = value\n",
    "        return out\n",
    "\n",
    "    def _singleton_to_pz_dict(self, value: bool) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Broadcasts the `done` and `trunc` flags to dictionaries keyed by agent id.\n",
    "        \"\"\"\n",
    "        return {str(agent_index): value for agent_index in range(self.n_agents)}\n",
    "\n",
    "    def reset(self) -> Tuple[Dict[AgentID, ObsType], Dict[AgentID, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment and returns a dictionary of observations\n",
    "        keyed by agent ID.\n",
    "        \"\"\"\n",
    "        obs = self.env.reset()  # random level generation\n",
    "        pz_obs = self._np_array_to_pz_dict(obs[\"image\"])\n",
    "\n",
    "        return pz_obs\n",
    "\n",
    "    def step(self, action: Dict[AgentID, ActionType], device: str) -> Tuple[\n",
    "        Dict[AgentID, ObsType],\n",
    "        Dict[AgentID, float],\n",
    "        Dict[AgentID, bool],\n",
    "        Dict[AgentID, bool],\n",
    "        Dict[AgentID, dict],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Takes inputs in the PettingZoo (PZ) Parallel API format, performs a step and\n",
    "        returns outputs in PZ format.\n",
    "        \"\"\"\n",
    "        action = batchify(action, device)\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        obs = obs[\"image\"]\n",
    "        trunc = False  # there is no `truncated` flag in this environment\n",
    "        self.task_completion = self._task_completion(obs, rew, done, trunc, info)\n",
    "        # convert outputs back to PZ format\n",
    "        obs, rew = map(self._np_array_to_pz_dict, [obs, rew])\n",
    "        done, trunc, info = map(\n",
    "            self._singleton_to_pz_dict, [done, trunc, self.task_completion]\n",
    "        )\n",
    "        self.n_steps += 1\n",
    "\n",
    "        return self.observation(obs), rew, done, trunc, info\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    agent_curriculum: str\n",
    "    env_curriculum: str\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.env_curriculum}_{self.agent_curriculum}\"\n",
    "\n",
    "\n",
    "def load_agent(\n",
    "    agent_config: AgentConfig,\n",
    "    step: int,\n",
    "    seed: int,\n",
    "    device: str = \"cpu\",\n",
    ") -> AgentType:\n",
    "\n",
    "    return joblib.load(\n",
    "        (\n",
    "            f\"lasertag_{str(agent_config)}_checkpoints/\"\n",
    "            f\"{str(agent_config)}_{step}_seed_{seed}.pkl\"\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def play_n_episodes(\n",
    "    agent_1_config: Dict[str, Union[AgentCurriculum, EnvCurriculum]],\n",
    "    agent_2_config: Dict[str, Union[AgentCurriculum, EnvCurriculum]],\n",
    "    step: int,\n",
    "    seed: int,\n",
    "    n_episodes: int = 10,\n",
    "    environment_id: str = \"LasertagArena1\",\n",
    ") -> Dict[str, int]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = test_envs[environment_id]()  # 2 agents by default\n",
    "    env = LasertagFixedParallelWrapper(env=env, n_agents=2)\n",
    "\n",
    "    agent_1 = load_agent(agent_config=agent_1_config, step=step, seed=seed)\n",
    "    agent_2 = load_agent(agent_config=agent_2_config, step=step, seed=seed)\n",
    "\n",
    "    stack_size = 3\n",
    "    frame_size = (env.agent_view_size, env.agent_view_size)\n",
    "    max_cycles = env.max_steps\n",
    "    n_agents = 2\n",
    "    agent_c_rew, opp_c_rew = 0, 0\n",
    "\n",
    "    \"\"\"ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    rb_obs = torch.zeros((max_cycles, n_agents, stack_size, *frame_size)).to(device)\n",
    "    rb_actions = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "    rb_terms = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "\n",
    "    \"\"\" TRAINING LOGIC \"\"\"\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        # collect an episode\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_obs = env.reset()\n",
    "\n",
    "            # each episode has num_steps\n",
    "            for step in range(0, max_cycles):\n",
    "                # rollover the observation\n",
    "                joint_obs = batchify(next_obs, device).squeeze()\n",
    "                agent_obs, opponent_obs = joint_obs\n",
    "\n",
    "                # get action from the agent and the opponent\n",
    "                agent_1_action, *_ = agent_1.get_action_and_value(\n",
    "                    agent_obs, flatten_start_dim=0\n",
    "                )\n",
    "                agent_2_action, *_ = agent_2.get_action_and_value(\n",
    "                    opponent_obs, flatten_start_dim=0\n",
    "                )\n",
    "                # execute the environment and log data\n",
    "                joint_actions = torch.tensor((agent_1_action, agent_2_action))\n",
    "                next_obs, rewards, terms, truncs, info = env.step(\n",
    "                    unbatchify(joint_actions, env.possible_agents), device\n",
    "                )\n",
    "\n",
    "                agent_c_rew += rewards[\"agent_0\"]\n",
    "                opp_c_rew += rewards[\"agent_1\"]\n",
    "\n",
    "                # add to episode storage\n",
    "                rb_obs[step] = batchify(next_obs, device)\n",
    "                rb_rewards[step] = batchify(rewards, device)\n",
    "                rb_terms[step] = batchify(terms, device)\n",
    "                rb_actions[step] = joint_actions\n",
    "\n",
    "                # if we reach termination or truncation, end\n",
    "                if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                    break\n",
    "\n",
    "    return {\n",
    "        f\"agent_1_step_{step}_seed_{seed}_rewards\": agent_c_rew / n_episodes,\n",
    "        f\"agent_2_step_{step}_seed_{seed}_rewards\": opp_c_rew / n_episodes,\n",
    "    }\n",
    "\n",
    "\n",
    "test_envs = {\n",
    "    \"LasertagArena1\": LasertagArena1,\n",
    "    \"LasertagArena2\": LasertagArena2,\n",
    "    \"LasertagCorridor1\": LasertagCorridor1,\n",
    "    \"LasertagCorridor2\": LasertagCorridor2,\n",
    "    \"LasertagMaze1\": LasertagMaze1,\n",
    "    \"LasertagMaze2\": LasertagMaze2,\n",
    "    \"LasertagRuins\": LasertagRuins,\n",
    "    \"LasertagRuins2\": LasertagRuins2,\n",
    "    \"LasertagStar\": LasertagStar,\n",
    "    \"LasertagCross\": LasertagCross,\n",
    "    \"LasertagLargeCorridor\": LasertagLargeCorridor,\n",
    "    \"LasertagSixteenRoomsN2\": LasertagSixteenRoomsN2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_1_step_199_seed_1_rewards': 0.01, 'agent_2_step_199_seed_1_rewards': -0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_1_step_199_seed_1_rewards': -0.01, 'agent_2_step_199_seed_1_rewards': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_1_step_199_seed_1_rewards': -0.02, 'agent_2_step_199_seed_1_rewards': 0.02}\n",
      "{'DR_FSP_VS_DR_FSP_checkpoint_2000_seed_1': {'agent_1_step_199_seed_1_rewards': 0.01, 'agent_2_step_199_seed_1_rewards': -0.01}, 'DR_FSP_VS_DR_FSP_checkpoint_4000_seed_1': {'agent_1_step_199_seed_1_rewards': -0.01, 'agent_2_step_199_seed_1_rewards': 0.01}, 'DR_FSP_VS_DR_FSP_checkpoint_6000_seed_1': {'agent_1_step_199_seed_1_rewards': -0.02, 'agent_2_step_199_seed_1_rewards': 0.02}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class args:\n",
    "    agent_curriculum_1 = \"FSP\"\n",
    "    agent_curriculum_2 = \"FSP\"\n",
    "    env_curriculum_1 = \"DR\"\n",
    "    env_curriculum_2 = \"DR\"\n",
    "    logging_dir = \".\"\n",
    "    n_episodes = 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # args = parse_args()\n",
    "    agent_1_config = AgentConfig(args.agent_curriculum_1, args.env_curriculum_1)\n",
    "    agent_2_config = AgentConfig(args.agent_curriculum_2, args.env_curriculum_2)\n",
    "\n",
    "    if not os.path.exists(f\"{args.logging_dir}\"):\n",
    "        os.makedirs(f\"{args.logging_dir}\", exist_ok=True)\n",
    "\n",
    "    logs = {}\n",
    "\n",
    "    for checkpoint in [2000, 4000, 6000]:\n",
    "        for seed in [1]:\n",
    "            returns = play_n_episodes(\n",
    "                agent_1_config=agent_1_config,\n",
    "                agent_2_config=agent_2_config,\n",
    "                step=checkpoint,\n",
    "                seed=seed,\n",
    "                n_episodes=args.n_episodes,\n",
    "                environment_id=\"LasertagArena1\",\n",
    "            )\n",
    "            print(returns)\n",
    "            title = (\n",
    "                f\"{str(agent_1_config)}_VS_{str(agent_2_config)}_\"\n",
    "                f\"checkpoint_{checkpoint}_seed_{seed}\"\n",
    "            )\n",
    "            logs[title] = returns\n",
    "\n",
    "    print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DR_FSP_VS_DR_FSP_checkpoint_2000_seed_1</th>\n",
       "      <th>DR_FSP_VS_DR_FSP_checkpoint_4000_seed_1</th>\n",
       "      <th>DR_FSP_VS_DR_FSP_checkpoint_6000_seed_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agent_1_step_199_seed_1_rewards</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agent_2_step_199_seed_1_rewards</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 DR_FSP_VS_DR_FSP_checkpoint_2000_seed_1  \\\n",
       "agent_1_step_199_seed_1_rewards                                     0.01   \n",
       "agent_2_step_199_seed_1_rewards                                    -0.01   \n",
       "\n",
       "                                 DR_FSP_VS_DR_FSP_checkpoint_4000_seed_1  \\\n",
       "agent_1_step_199_seed_1_rewards                                    -0.01   \n",
       "agent_2_step_199_seed_1_rewards                                     0.01   \n",
       "\n",
       "                                 DR_FSP_VS_DR_FSP_checkpoint_6000_seed_1  \n",
       "agent_1_step_199_seed_1_rewards                                    -0.02  \n",
       "agent_2_step_199_seed_1_rewards                                     0.02  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
