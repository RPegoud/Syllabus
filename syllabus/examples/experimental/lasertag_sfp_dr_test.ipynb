{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import TypeVar\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from gymnasium import spaces\n",
    "from plotly.subplots import make_subplots\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"../../..\")\n",
    "from lasertag import LasertagAdversarial  # noqa: E402\n",
    "from syllabus.core import (  # noqa: E402\n",
    "    DualCurriculumWrapper,\n",
    "    TaskWrapper,\n",
    "    make_multiprocessing_curriculum,\n",
    ")\n",
    "\n",
    "# noqa: E402\n",
    "from syllabus.curricula import (  # noqa: E402\n",
    "    DomainRandomization,\n",
    "    PrioritizedFictitiousSelfPlay,\n",
    ")\n",
    "from syllabus.task_space import TaskSpace  # noqa: E402\n",
    "\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "Agent = TypeVar(\"Agent\")\n",
    "EnvTask = TypeVar(\"EnvTask\")\n",
    "AgentTask = TypeVar(\"AgentTask\")\n",
    "ObsType = TypeVar(\"ObsType\")\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, possible_agents: np.ndarray):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {agent: x[idx] for idx, agent in enumerate(possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class LasertagParallelWrapper(TaskWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper ensuring compatibility with the PettingZoo Parallel API.\n",
    "\n",
    "    Lasertag Environment:\n",
    "        * Action shape:  `n_agents` * `Discrete(5)`\n",
    "        * Observation shape: Dict('image': Box(0, 255, (`n_agents`, 3, 5, 5), uint8))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_agents = n_agents\n",
    "        self.task = None\n",
    "        self.episode_return = 0\n",
    "        self.task_space = TaskSpace(spaces.MultiDiscrete(np.array([[2], [5]])))\n",
    "        self.possible_agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Delegate attribute lookup to the wrapped environment if the attribute\n",
    "        is not found in the LasertagParallelWrapper instance.\n",
    "        \"\"\"\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def _np_array_to_pz_dict(self, array: np.ndarray) -> dict[str : np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing individual observations for each agent.\n",
    "        Assumes that the batch dimension represents individual agents.\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        for idx, value in enumerate(array):\n",
    "            out[self.possible_agents[idx]] = value\n",
    "        return out\n",
    "\n",
    "    def _singleton_to_pz_dict(self, value: bool) -> dict[str:bool]:\n",
    "        \"\"\"\n",
    "        Broadcasts the `done` and `trunc` flags to dictionaries keyed by agent id.\n",
    "        \"\"\"\n",
    "        return {str(agent_index): value for agent_index in range(self.n_agents)}\n",
    "\n",
    "    def reset(\n",
    "        self, env_task: int\n",
    "    ) -> tuple[dict[AgentID, ObsType], dict[AgentID, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment and returns a dictionary of observations\n",
    "        keyed by agent ID.\n",
    "        \"\"\"\n",
    "        self.env.seed(env_task)\n",
    "        obs = self.env.reset_random()  # random level generation\n",
    "        pz_obs = self._np_array_to_pz_dict(obs[\"image\"])\n",
    "\n",
    "        return pz_obs\n",
    "\n",
    "    def step(\n",
    "        self, action: dict[AgentID, ActionType], device: str, agent_task: int\n",
    "    ) -> tuple[\n",
    "        dict[AgentID, ObsType],\n",
    "        dict[AgentID, float],\n",
    "        dict[AgentID, bool],\n",
    "        dict[AgentID, bool],\n",
    "        dict[AgentID, dict],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Takes inputs in the PettingZoo (PZ) Parallel API format, performs a step and\n",
    "        returns outputs in PZ format.\n",
    "        \"\"\"\n",
    "        action = batchify(action, device)\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        obs = obs[\"image\"]\n",
    "        trunc = False  # there is no `truncated` flag in this environment\n",
    "        self.task_completion = self._task_completion(obs, rew, done, trunc, info)\n",
    "        # convert outputs back to PZ format\n",
    "        obs, rew = map(self._np_array_to_pz_dict, [obs, rew])\n",
    "        done, trunc, info = map(\n",
    "            self._singleton_to_pz_dict, [done, trunc, self.task_completion]\n",
    "        )\n",
    "        info[\"agent_id\"] = agent_task\n",
    "\n",
    "        return self.observation(obs), rew, done, trunc, info\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Linear(3 * 5 * 5, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "\"\"\"ALGO PARAMS\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.0\n",
    "vf_coef = 0.5\n",
    "clip_coef = 0.2\n",
    "learning_rate = 1e-4\n",
    "epsilon = 1e-5\n",
    "gamma = 0.995\n",
    "gae_lambda = 0.95\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "stack_size = 3\n",
    "frame_size = (5, 5)\n",
    "max_cycles = 201  # lasertag has 200 maximum steps by default\n",
    "total_episodes = 500\n",
    "n_agents = 2\n",
    "num_actions = 5\n",
    "fsp_update_frequency = 50\n",
    "\n",
    "save_agent_checkpoints = True\n",
    "checkpoint_frequency = total_episodes / 10\n",
    "logging_dir = \"./pfsp_checkpoints\"\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "agent = Agent(num_actions=num_actions).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, TypeVar\n",
    "\n",
    "from syllabus.core.curriculum_base import Curriculum\n",
    "from syllabus.core.task_interface import TaskWrapper\n",
    "\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "Agent = TypeVar(\"Agent\")\n",
    "EnvTask = TypeVar(\"EnvTask\")\n",
    "AgentTask = TypeVar(\"AgentTask\")\n",
    "\n",
    "\n",
    "class DualCurriculumWrapper(Curriculum):\n",
    "    \"\"\"Curriculum wrapper containing both an agent and environment-based curriculum.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: TaskWrapper,\n",
    "        env_curriculum: Curriculum,\n",
    "        agent_curriculum: Curriculum,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.env = env\n",
    "        self.agent_curriculum = agent_curriculum\n",
    "        self.env_curriculum = env_curriculum\n",
    "        self.task_space = (env_curriculum.task_space, agent_curriculum.task_space)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def sample(self, k=1) -> Tuple[EnvTask, AgentTask]:\n",
    "        \"\"\"Sets new tasks for the environment and agent curricula.\"\"\"\n",
    "        self.env_task = self.env_curriculum.sample()\n",
    "        self.agent_task = self.agent_curriculum.sample()\n",
    "        return self.env_task, self.agent_task\n",
    "\n",
    "    def get_opponent(self, agent_task: AgentTask) -> Agent:\n",
    "        return self.agent_curriculum.get_opponent(agent_task)\n",
    "\n",
    "    def update_agent(self, agent: Agent) -> Agent:\n",
    "        return self.agent_curriculum.update_agent(agent)\n",
    "\n",
    "    def update_on_step(self, task, step, reward, term, trunc):\n",
    "        self.env_curriculum.update_on_step(task, step, reward, term, trunc)\n",
    "        self.agent_curriculum.update_on_step(task, step, reward, term, trunc)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"Delegate attribute lookup to the curricula if not found.\"\"\"\n",
    "        if hasattr(self.env_curriculum, name):\n",
    "            return getattr(self.env_curriculum, name)\n",
    "        elif hasattr(self.agent_curriculum, name):\n",
    "            return getattr(self.agent_curriculum, name)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.Agent'>: it's not the same object as __main__.Agent",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m LasertagParallelWrapper(env\u001b[38;5;241m=\u001b[39menv, n_agents\u001b[38;5;241m=\u001b[39mn_agents)\n\u001b[0;32m      5\u001b[0m env_curriculum \u001b[38;5;241m=\u001b[39m DomainRandomization(TaskSpace(spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;241m200\u001b[39m)))\n\u001b[1;32m----> 6\u001b[0m agent_curriculum \u001b[38;5;241m=\u001b[39m \u001b[43mPrioritizedFictitiousSelfPlay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpfsp_agents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m dual_curriculum \u001b[38;5;241m=\u001b[39m DualCurriculumWrapper(\n\u001b[0;32m     10\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m     11\u001b[0m     agent_curriculum\u001b[38;5;241m=\u001b[39magent_curriculum,\n\u001b[0;32m     12\u001b[0m     env_curriculum\u001b[38;5;241m=\u001b[39menv_curriculum,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m curriculum, task_queue, update_queue \u001b[38;5;241m=\u001b[39m make_multiprocessing_curriculum(dual_curriculum)\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\OneDrive\\Documents\\Syllabus-1\\syllabus\\examples\\experimental\\../../..\\syllabus\\curricula\\selfplay.py:103\u001b[0m, in \u001b[0;36mPrioritizedFictitiousSelfPlay.__init__\u001b[1;34m(self, agent, device, storage_path, max_agents)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_agents \u001b[38;5;241m=\u001b[39m max_agents\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_space \u001b[38;5;241m=\u001b[39m TaskSpace(spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_agents))\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# creates the initial opponent\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    105\u001b[0m     i: {\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwinrate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_agents)\n\u001b[0;32m    110\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\OneDrive\\Documents\\Syllabus-1\\syllabus\\examples\\experimental\\../../..\\syllabus\\curricula\\selfplay.py:117\u001b[0m, in \u001b[0;36mPrioritizedFictitiousSelfPlay.update_agent\u001b[1;34m(self, agent)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_agent\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    Saves the current agent instance to a pickle file and update\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    its priority.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_agent_checkpoint_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_agent_index\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_agents\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_agent_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\numpy_pickle.py:553\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 553\u001b[0m         \u001b[43mNumpyPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    555\u001b[0m     NumpyPickler(filename, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[1;32m--> 487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pickle.py:687\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs[0] from __newobj__ args has the wrong class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    686\u001b[0m args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 687\u001b[0m \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m save(args)\n\u001b[0;32m    689\u001b[0m write(NEWOBJ)\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pickle.py:1129\u001b[0m, in \u001b[0;36m_Pickler.save_type\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m):\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_reduce(\u001b[38;5;28mtype\u001b[39m, (\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,), obj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m-> 1129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_global\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pickle.py:1076\u001b[0m, in \u001b[0;36m_Pickler.save_global\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj:\n\u001b[1;32m-> 1076\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\n\u001b[0;32m   1077\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m: it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not the same object as \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1078\u001b[0m             (obj, module_name, name))\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1081\u001b[0m     code \u001b[38;5;241m=\u001b[39m _extension_registry\u001b[38;5;241m.\u001b[39mget((module_name, name))\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <class '__main__.Agent'>: it's not the same object as __main__.Agent"
     ]
    }
   ],
   "source": [
    "\"\"\" ENV SETUP \"\"\"\n",
    "\n",
    "env = LasertagAdversarial(record_video=False)  # 2 agents by default\n",
    "env = LasertagParallelWrapper(env=env, n_agents=n_agents)\n",
    "env_curriculum = DomainRandomization(TaskSpace(spaces.Discrete(200)))\n",
    "agent_curriculum = PrioritizedFictitiousSelfPlay(\n",
    "    agent=agent, device=device, storage_path=\"pfsp_agents\", max_agents=10\n",
    ")\n",
    "dual_curriculum = DualCurriculumWrapper(\n",
    "    env=env,\n",
    "    agent_curriculum=agent_curriculum,\n",
    "    env_curriculum=env_curriculum,\n",
    ")\n",
    "curriculum, task_queue, update_queue = make_multiprocessing_curriculum(dual_curriculum)\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, n_agents, stack_size, *frame_size)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "\n",
    "agent_tasks, env_tasks = [], []\n",
    "agent_c_rew, opp_c_rew = 0, 0\n",
    "n_ends, n_learner_wins = 0, 0\n",
    "info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "\n",
    "# train for n number of episodes\n",
    "for episode in tqdm(range(total_episodes)):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        env_task, agent_task = curriculum.sample()\n",
    "\n",
    "        env_tasks.append(env_task[0])\n",
    "        agent_tasks.append(agent_task)\n",
    "\n",
    "        next_obs = env.reset(env_task)\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "        n_steps = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            # rollover the observation\n",
    "            joint_obs = batchify(next_obs, device).squeeze()\n",
    "            agent_obs, opponent_obs = joint_obs\n",
    "\n",
    "            # get action from the agent and the opponent\n",
    "            actions, logprobs, _, values = agent.get_action_and_value(\n",
    "                agent_obs, flatten_start_dim=0\n",
    "            )\n",
    "\n",
    "            opponent = curriculum.get_opponent(info.get(\"agent_id\", 0)).to(device)\n",
    "            opponent_action, *_ = opponent.get_action_and_value(\n",
    "                opponent_obs, flatten_start_dim=0\n",
    "            )\n",
    "            # execute the environment and log data\n",
    "            joint_actions = torch.tensor((actions, opponent_action))\n",
    "            next_obs, rewards, terms, truncs, info = env.step(\n",
    "                unbatchify(joint_actions, env.possible_agents), device, agent_task\n",
    "            )\n",
    "\n",
    "            opp_reward = rewards[\"agent_1\"]\n",
    "            if opp_reward != 0:\n",
    "                n_ends += 1\n",
    "                curriculum.update_winrate(info[\"agent_id\"], opp_reward)\n",
    "                if opp_reward == -1:\n",
    "                    n_learner_wins += 1\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = batchify(next_obs, device)\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = joint_actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values.flatten()\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "            n_steps += 1\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(\n",
    "            torch.tensor(next_obs[\"agent_0\"]), flatten_start_dim=0\n",
    "        )\n",
    "        rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "        last_gae_lam = 0\n",
    "        for t in reversed(range(end_step)):\n",
    "            if t == end_step - 1:\n",
    "                next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                next_values = rb_values[t + 1]\n",
    "            delta = (\n",
    "                rb_rewards[t] + gamma * next_values * next_non_terminal - rb_values[t]\n",
    "            )\n",
    "            rb_advantages[t] = last_gae_lam = (\n",
    "                delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            )\n",
    "        rb_returns = rb_advantages + rb_values\n",
    "    # convert our episodes to batch of individual transitions\n",
    "    b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "    b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "    b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "    b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_index = np.arange(len(b_obs))\n",
    "    clip_fracs = []\n",
    "    for repeat in range(epochs):\n",
    "        # shuffle the indices we use to access the data\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_obs), batch_size):\n",
    "            # select the indices we want to train on\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                b_obs[batch_index], b_actions.long()[batch_index]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            # normalize advantages\n",
    "            rb_advantages = b_advantages[batch_index]\n",
    "            rb_advantages = (rb_advantages - rb_advantages.mean()) / (\n",
    "                rb_advantages.std() + 1e-8\n",
    "            )\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "            pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                ratio, 1 - clip_coef, 1 + clip_coef\n",
    "            )\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # update opponent\n",
    "    if episode % fsp_update_frequency == 0 and episode != 0:\n",
    "        curriculum.update_agent(agent)\n",
    "\n",
    "    # store learner checkpoints\n",
    "    if save_agent_checkpoints:\n",
    "        if n_steps % checkpoint_frequency == 0:\n",
    "            joblib.dump(\n",
    "                agent,\n",
    "                f\"{logging_dir}/agent_checkpoints/{env_curriculum_name}_{curriculum.agent_curriculum.name}_{step}\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'curriculum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcurriculum\u001b[49m\u001b[38;5;241m.\u001b[39mget_opponent(info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'curriculum' is not defined"
     ]
    }
   ],
   "source": [
    "curriculum.get_opponent(info.get(\"agent_id\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces.dict import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m gym_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mDict(\n\u001b[0;32m      2\u001b[0m     {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_space\u001b[39m\u001b[38;5;124m\"\u001b[39m: env_curriculum\u001b[38;5;241m.\u001b[39mtask_space\u001b[38;5;241m.\u001b[39mgym_space,\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_space\u001b[39m\u001b[38;5;124m\"\u001b[39m: env_curriculum\u001b[38;5;241m.\u001b[39mtask_space\u001b[38;5;241m.\u001b[39mgym_space,\n\u001b[0;32m      5\u001b[0m     }\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mTaskSpace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_curriculum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_curriculum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\OneDrive\\Documents\\Syllabus-1\\syllabus\\examples\\experimental\\../../..\\syllabus\\task_space\\task_space.py:29\u001b[0m, in \u001b[0;36mTaskSpace.__init__\u001b[1;34m(self, gym_space, tasks)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m         tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(gym_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_task_encoder(gym_space, tasks)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "gym_space = spaces.Dict(\n",
    "    {\n",
    "        \"agent_space\": env_curriculum.task_space.gym_space,\n",
    "        \"env_space\": env_curriculum.task_space.gym_space,\n",
    "    }\n",
    ")\n",
    "\n",
    "TaskSpace(gym_space, [env_curriculum.tasks, env_curriculum.tasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m gym_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mTuple(\n\u001b[0;32m      2\u001b[0m     (\n\u001b[0;32m      3\u001b[0m         env_curriculum\u001b[38;5;241m.\u001b[39mtask_space\u001b[38;5;241m.\u001b[39mgym_space,\n\u001b[0;32m      4\u001b[0m         env_curriculum\u001b[38;5;241m.\u001b[39mtask_space\u001b[38;5;241m.\u001b[39mgym_space,\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m gym_space\n\u001b[1;32m----> 9\u001b[0m \u001b[43mTaskSpace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_curriculum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_curriculum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\OneDrive\\Documents\\Syllabus-1\\syllabus\\examples\\experimental\\../../..\\syllabus\\task_space\\task_space.py:29\u001b[0m, in \u001b[0;36mTaskSpace.__init__\u001b[1;34m(self, gym_space, tasks)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m         tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(gym_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_task_encoder(gym_space, tasks)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "gym_space = spaces.Tuple(\n",
    "    (\n",
    "        env_curriculum.task_space.gym_space,\n",
    "        env_curriculum.task_space.gym_space,\n",
    "    )\n",
    ")\n",
    "gym_space\n",
    "\n",
    "TaskSpace(gym_space, [env_curriculum.tasks, env_curriculum.tasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTaskSpace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_curriculum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_curriculum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\OneDrive\\Documents\\Syllabus-1\\syllabus\\examples\\experimental\\../../..\\syllabus\\task_space\\task_space.py:29\u001b[0m, in \u001b[0;36mTaskSpace.__init__\u001b[1;34m(self, gym_space, tasks)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m         tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(gym_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_task_encoder(gym_space, tasks)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "TaskSpace(gym_space, (env_curriculum.tasks, env_curriculum.tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 agent_space Discrete(200)\n",
      "1\n",
      "1 env_space Discrete(200)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(gym_space.items()):\n",
    "    print(i, k, v)\n",
    "    encoder, decoder = TaskSpace._make_task_encoder(\n",
    "        TaskSpace, gym_space, env_curriculum.tasks\n",
    "    )\n",
    "    print(encoder(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
