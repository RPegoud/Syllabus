{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import TypeVar\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from gymnasium import spaces\n",
    "from plotly.subplots import make_subplots\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"../../..\")\n",
    "from lasertag import LasertagAdversarial  # noqa: E402\n",
    "from syllabus.core import (  # noqa: E402\n",
    "    DualCurriculumWrapper,\n",
    "    TaskWrapper,\n",
    "    make_multiprocessing_curriculum,\n",
    ")\n",
    "\n",
    "# noqa: E402\n",
    "from syllabus.curricula import (  # noqa: E402\n",
    "    DomainRandomization,\n",
    "    PrioritizedFictitiousSelfPlay,\n",
    ")\n",
    "from syllabus.task_space import TaskSpace  # noqa: E402\n",
    "\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "Agent = TypeVar(\"Agent\")\n",
    "EnvTask = TypeVar(\"EnvTask\")\n",
    "AgentTask = TypeVar(\"AgentTask\")\n",
    "ObsType = TypeVar(\"ObsType\")\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, possible_agents: np.ndarray):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {agent: x[idx] for idx, agent in enumerate(possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class LasertagParallelWrapper(TaskWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper ensuring compatibility with the PettingZoo Parallel API.\n",
    "\n",
    "    Lasertag Environment:\n",
    "        * Action shape:  `n_agents` * `Discrete(5)`\n",
    "        * Observation shape: Dict('image': Box(0, 255, (`n_agents`, 3, 5, 5), uint8))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_agents = n_agents\n",
    "        self.task = None\n",
    "        self.episode_return = 0\n",
    "        self.possible_agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Delegate attribute lookup to the wrapped environment if the attribute\n",
    "        is not found in the LasertagParallelWrapper instance.\n",
    "        \"\"\"\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def _np_array_to_pz_dict(self, array: np.ndarray) -> dict[str : np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing individual observations for each agent.\n",
    "        Assumes that the batch dimension represents individual agents.\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        for idx, value in enumerate(array):\n",
    "            out[self.possible_agents[idx]] = value\n",
    "        return out\n",
    "\n",
    "    def _singleton_to_pz_dict(self, value: bool) -> dict[str:bool]:\n",
    "        \"\"\"\n",
    "        Broadcasts the `done` and `trunc` flags to dictionaries keyed by agent id.\n",
    "        \"\"\"\n",
    "        return {str(agent_index): value for agent_index in range(self.n_agents)}\n",
    "\n",
    "    def reset(\n",
    "        self, env_task: int\n",
    "    ) -> tuple[dict[AgentID, ObsType], dict[AgentID, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment and returns a dictionary of observations\n",
    "        keyed by agent ID.\n",
    "        \"\"\"\n",
    "        self.env.seed(env_task)\n",
    "        obs = self.env.reset_random()  # random level generation\n",
    "        pz_obs = self._np_array_to_pz_dict(obs[\"image\"])\n",
    "\n",
    "        return pz_obs\n",
    "\n",
    "    def step(\n",
    "        self, action: dict[AgentID, ActionType], device: str, agent_task: int\n",
    "    ) -> tuple[\n",
    "        dict[AgentID, ObsType],\n",
    "        dict[AgentID, float],\n",
    "        dict[AgentID, bool],\n",
    "        dict[AgentID, bool],\n",
    "        dict[AgentID, dict],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Takes inputs in the PettingZoo (PZ) Parallel API format, performs a step and\n",
    "        returns outputs in PZ format.\n",
    "        \"\"\"\n",
    "        action = batchify(action, device)\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        obs = obs[\"image\"]\n",
    "        trunc = False  # there is no `truncated` flag in this environment\n",
    "        self.task_completion = self._task_completion(obs, rew, done, trunc, info)\n",
    "        # convert outputs back to PZ format\n",
    "        obs, rew = map(self._np_array_to_pz_dict, [obs, rew])\n",
    "        done, trunc, info = map(\n",
    "            self._singleton_to_pz_dict, [done, trunc, self.task_completion]\n",
    "        )\n",
    "        info[\"agent_id\"] = agent_task\n",
    "\n",
    "        return self.observation(obs), rew, done, trunc, info\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Linear(3 * 5 * 5, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "\"\"\"ALGO PARAMS\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.0\n",
    "vf_coef = 0.5\n",
    "clip_coef = 0.2\n",
    "learning_rate = 1e-4\n",
    "epsilon = 1e-5\n",
    "gamma = 0.995\n",
    "gae_lambda = 0.95\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "stack_size = 3\n",
    "frame_size = (5, 5)\n",
    "max_cycles = 201  # lasertag has 200 maximum steps by default\n",
    "total_episodes = 500\n",
    "n_agents = 2\n",
    "num_actions = 5\n",
    "fsp_update_frequency = 50\n",
    "\n",
    "save_agent_checkpoints = True\n",
    "checkpoint_frequency = total_episodes / 10\n",
    "logging_dir = \"./pfsp_checkpoints\"\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "agent = Agent(num_actions=num_actions).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ENV SETUP \"\"\"\n",
    "\n",
    "env = LasertagAdversarial(record_video=False)  # 2 agents by default\n",
    "env = LasertagParallelWrapper(env=env, n_agents=n_agents)\n",
    "env_curriculum = DomainRandomization(TaskSpace(spaces.Discrete(200)))\n",
    "agent_curriculum = PrioritizedFictitiousSelfPlay(\n",
    "    agent=agent, device=device, storage_path=\"pfsp_agents\", max_agents=10\n",
    ")\n",
    "dual_curriculum = DualCurriculumWrapper(\n",
    "    env=env,\n",
    "    agent_curriculum=agent_curriculum,\n",
    "    env_curriculum=env_curriculum,\n",
    ")\n",
    "curriculum = make_multiprocessing_curriculum(dual_curriculum)\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, n_agents, stack_size, *frame_size)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "\n",
    "agent_tasks, env_tasks = [], []\n",
    "agent_c_rew, opp_c_rew = 0, 0\n",
    "n_ends, n_learner_wins = 0, 0\n",
    "info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curriculum.curriculum.agent_curriculum.current_agent_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2389114725.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    assert\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "\n",
    "# train for n number of episodes\n",
    "for episode in tqdm(range(total_episodes)):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        env_task, agent_task = curriculum.sample()\n",
    "\n",
    "        env_tasks.append(env_task[0])\n",
    "        agent_tasks.append(agent_task)\n",
    "\n",
    "        next_obs = env.reset(env_task)\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "        n_steps = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            # rollover the observation\n",
    "            joint_obs = batchify(next_obs, device).squeeze()\n",
    "            agent_obs, opponent_obs = joint_obs\n",
    "\n",
    "            # get action from the agent and the opponent\n",
    "            actions, logprobs, _, values = agent.get_action_and_value(\n",
    "                agent_obs, flatten_start_dim=0\n",
    "            )\n",
    "\n",
    "            opponent = curriculum.get_opponent(info.get(\"agent_id\", 0)).to(device)\n",
    "            opponent_action, *_ = opponent.get_action_and_value(\n",
    "                opponent_obs, flatten_start_dim=0\n",
    "            )\n",
    "            # execute the environment and log data\n",
    "            joint_actions = torch.tensor((actions, opponent_action))\n",
    "            next_obs, rewards, terms, truncs, info = env.step(\n",
    "                unbatchify(joint_actions, env.possible_agents), device, agent_task\n",
    "            )\n",
    "\n",
    "            opp_reward = rewards[\"agent_1\"]\n",
    "            if opp_reward != 0:\n",
    "                n_ends += 1\n",
    "                curriculum.update_winrate(info[\"agent_id\"], opp_reward)\n",
    "                if opp_reward == -1:\n",
    "                    n_learner_wins += 1\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = batchify(next_obs, device)\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = joint_actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values.flatten()\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "            n_steps += 1\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(\n",
    "            torch.tensor(next_obs[\"agent_0\"]), flatten_start_dim=0\n",
    "        )\n",
    "        rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "        last_gae_lam = 0\n",
    "        for t in reversed(range(end_step)):\n",
    "            if t == end_step - 1:\n",
    "                next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                next_values = rb_values[t + 1]\n",
    "            delta = (\n",
    "                rb_rewards[t] + gamma * next_values * next_non_terminal - rb_values[t]\n",
    "            )\n",
    "            rb_advantages[t] = last_gae_lam = (\n",
    "                delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            )\n",
    "        rb_returns = rb_advantages + rb_values\n",
    "    # convert our episodes to batch of individual transitions\n",
    "    b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "    b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "    b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "    b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_index = np.arange(len(b_obs))\n",
    "    clip_fracs = []\n",
    "    for repeat in range(epochs):\n",
    "        # shuffle the indices we use to access the data\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_obs), batch_size):\n",
    "            # select the indices we want to train on\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                b_obs[batch_index], b_actions.long()[batch_index]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            # normalize advantages\n",
    "            rb_advantages = b_advantages[batch_index]\n",
    "            rb_advantages = (rb_advantages - rb_advantages.mean()) / (\n",
    "                rb_advantages.std() + 1e-8\n",
    "            )\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "            pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                ratio, 1 - clip_coef, 1 + clip_coef\n",
    "            )\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # update opponent\n",
    "    if episode % fsp_update_frequency == 0 and episode != 0:\n",
    "        curriculum.update_agent(agent)\n",
    "\n",
    "    # store learner checkpoints\n",
    "    if save_agent_checkpoints:\n",
    "        if n_steps % checkpoint_frequency == 0:\n",
    "            joblib.dump(\n",
    "                agent,\n",
    "                f\"{logging_dir}/agent_checkpoints/{env_curriculum_name}_{curriculum.agent_curriculum.name}_{step}\",\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
