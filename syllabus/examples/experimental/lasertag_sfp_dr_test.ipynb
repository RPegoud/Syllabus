{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import TypeVar\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from gymnasium import spaces\n",
    "from plotly.subplots import make_subplots\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from lasertag_dr import LasertagParallelWrapper\n",
    "sys.path.append(\"../../..\")\n",
    "from lasertag import LasertagAdversarial  # noqa: E402\n",
    "from syllabus.core import (  # noqa: E402\n",
    "    DualCurriculumWrapper,\n",
    "    TaskWrapper,\n",
    "    make_multiprocessing_curriculum,\n",
    ")\n",
    "\n",
    "# noqa: E402\n",
    "from syllabus.curricula import (  # noqa: E402\n",
    "    DomainRandomization,\n",
    "    PrioritizedFictitiousSelfPlay,\n",
    ")\n",
    "from syllabus.task_space import TaskSpace  # noqa: E402\n",
    "\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "AgentType = TypeVar(\"AgentType\")\n",
    "EnvTask = TypeVar(\"EnvTask\")\n",
    "AgentTask = TypeVar(\"AgentTask\")\n",
    "ObsType = TypeVar(\"ObsType\")\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, possible_agents: np.ndarray):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {agent: x[idx] for idx, agent in enumerate(possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Linear(3 * 5 * 5, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "\"\"\"ALGO PARAMS\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.0\n",
    "vf_coef = 0.5\n",
    "clip_coef = 0.2\n",
    "learning_rate = 1e-4\n",
    "epsilon = 1e-5\n",
    "gamma = 0.995\n",
    "gae_lambda = 0.95\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "stack_size = 3\n",
    "frame_size = (5, 5)\n",
    "max_cycles = 201  # lasertag has 200 maximum steps by default\n",
    "total_episodes = 500\n",
    "n_agents = 2\n",
    "num_actions = 5\n",
    "fsp_update_frequency = 50\n",
    "\n",
    "save_agent_checkpoints = True\n",
    "checkpoint_frequency = total_episodes / 10\n",
    "logging_dir = \"./pfsp_checkpoints\"\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "agent = Agent(num_actions=num_actions).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ENV SETUP \"\"\"\n",
    "\n",
    "env = LasertagAdversarial(record_video=False)  # 2 agents by default\n",
    "env = LasertagParallelWrapper(env=env, n_agents=n_agents)\n",
    "env_curriculum = DomainRandomization(TaskSpace(spaces.Discrete(200)))\n",
    "agent_curriculum = PrioritizedFictitiousSelfPlay(\n",
    "    agent=agent, device=device, storage_path=\"pfsp_agents\", max_agents=10\n",
    ")\n",
    "dual_curriculum = DualCurriculumWrapper(\n",
    "    env=env,\n",
    "    agent_curriculum=agent_curriculum,\n",
    "    env_curriculum=env_curriculum,\n",
    ")\n",
    "mp_curriculum = make_multiprocessing_curriculum(dual_curriculum)\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, n_agents, stack_size, *frame_size)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "\n",
    "agent_tasks, env_tasks = [], []\n",
    "agent_c_rew, opp_c_rew = 0, 0\n",
    "n_ends, n_learner_wins = 0, 0\n",
    "info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82e91b06fd542d5bab7555dceddc7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint --6000\n",
      "saving checkpoint --8000\n",
      "saving checkpoint --10000\n",
      "saving checkpoint --12000\n",
      "saving checkpoint --14000\n",
      "saving checkpoint --16000\n",
      "saving checkpoint --18000\n",
      "saving checkpoint --20000\n",
      "saving checkpoint --22000\n",
      "saving checkpoint --24000\n",
      "saving checkpoint --26000\n",
      "saving checkpoint --28000\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class args:\n",
    "    total_episodes = 500\n",
    "    agent_curriculum = \"PFSP\"\n",
    "    logging_dir = \".\"\n",
    "\n",
    "\n",
    "# train for n number of episodes\n",
    "for episode in tqdm(range(args.total_episodes)):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        env_task, agent_task = mp_curriculum.sample()\n",
    "\n",
    "        env_tasks.append(env_task)\n",
    "        agent_tasks.append(agent_task)\n",
    "\n",
    "        next_obs = env.reset(env_task)\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            # rollover the observation\n",
    "            joint_obs = batchify(next_obs, device).squeeze()\n",
    "            agent_obs, opponent_obs = joint_obs\n",
    "\n",
    "            # get action from the agent and the opponent\n",
    "            actions, logprobs, _, values = agent.get_action_and_value(\n",
    "                agent_obs, flatten_start_dim=0\n",
    "            )\n",
    "\n",
    "            opponent = mp_curriculum.get_opponent(info.get(\"agent_id\", 0)).to(device)\n",
    "            opponent_action, *_ = opponent.get_action_and_value(\n",
    "                opponent_obs, flatten_start_dim=0\n",
    "            )\n",
    "            # execute the environment and log data\n",
    "            joint_actions = torch.tensor((actions, opponent_action))\n",
    "            next_obs, rewards, terms, truncs, info = env.step(\n",
    "                unbatchify(joint_actions, env.possible_agents), device, agent_task\n",
    "            )\n",
    "\n",
    "            opp_reward = rewards[\"agent_1\"]\n",
    "            if opp_reward != 0:\n",
    "                n_ends += 1\n",
    "                if args.agent_curriculum in [\"FSP\", \"PFSP\"]:\n",
    "                    mp_curriculum.update_winrate(info[\"agent_id\"], opp_reward)\n",
    "                if opp_reward == -1:\n",
    "                    n_learner_wins += 1\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = batchify(next_obs, device)\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = joint_actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values.flatten()\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "            # store learner checkpoints\n",
    "            if env.n_steps % 2000 == 0:\n",
    "                print(f\"saving checkpoint --{env.n_steps}\")\n",
    "                joblib.dump(\n",
    "                    agent,\n",
    "                    filename=(\n",
    "                        f\"{args.logging_dir}/test_checkpoints/\"\n",
    "                        f\"{mp_curriculum.curriculum.env_curriculum.name}_\"\n",
    "                        f\"{mp_curriculum.curriculum.agent_curriculum.name}_{env.n_steps}\"\n",
    "                        f\"_seed_{0}.pkl\"\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(\n",
    "            torch.tensor(next_obs[\"agent_0\"]), flatten_start_dim=0\n",
    "        )\n",
    "        rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "        last_gae_lam = 0\n",
    "        for t in reversed(range(end_step)):\n",
    "            if t == end_step - 1:\n",
    "                next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                next_values = rb_values[t + 1]\n",
    "            delta = (\n",
    "                rb_rewards[t] + gamma * next_values * next_non_terminal - rb_values[t]\n",
    "            )\n",
    "            rb_advantages[t] = last_gae_lam = (\n",
    "                delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            )\n",
    "        rb_returns = rb_advantages + rb_values\n",
    "    # convert our episodes to batch of individual transitions\n",
    "    b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "    b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "    b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "    b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_index = np.arange(len(b_obs))\n",
    "    clip_fracs = []\n",
    "    for repeat in range(epochs):\n",
    "        # shuffle the indices we use to access the data\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_obs), batch_size):\n",
    "            # select the indices we want to train on\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                b_obs[batch_index], b_actions.long()[batch_index]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            # normalize advantages\n",
    "            rb_advantages = b_advantages[batch_index]\n",
    "            rb_advantages = (rb_advantages - rb_advantages.mean()) / (\n",
    "                rb_advantages.std() + 1e-8\n",
    "            )\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "            pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                ratio, 1 - clip_coef, 1 + clip_coef\n",
    "            )\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # update opponent\n",
    "    if args.agent_curriculum in [\"FSP\", \"PFSP\"]:\n",
    "        if episode % 500 == 0 and episode != 0:\n",
    "            mp_curriculum.update_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=75, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (actor): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (critic): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.load(\"test_checkpoints/DR_PFSP_6000_seed_0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=75, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (actor): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (critic): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.load(\"lasertag_DR_SP_checkpoints/DR_SP_2000_seed_0.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
