{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, Tuple, TypeVar\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from gymnasium import spaces\n",
    "from plotly.subplots import make_subplots\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "sys.path.append(\"../../..\")\n",
    "from lasertag import LasertagAdversarial  # noqa: E402\n",
    "from syllabus.core import (  # noqa: E402\n",
    "    DualCurriculumWrapper,\n",
    "    TaskWrapper,\n",
    "    make_multiprocessing_curriculum,\n",
    ")\n",
    "\n",
    "# noqa: E402\n",
    "from syllabus.curricula import (  # noqa: E402\n",
    "    CentralizedPrioritizedLevelReplay,\n",
    "    DomainRandomization,\n",
    "    FictitiousSelfPlay,\n",
    "    PrioritizedFictitiousSelfPlay,\n",
    "    SelfPlay,\n",
    ")\n",
    "from syllabus.task_space import TaskSpace  # noqa: E402\n",
    "\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "AgentType = TypeVar(\"AgentType\")\n",
    "EnvTask = TypeVar(\"EnvTask\")\n",
    "AgentTask = TypeVar(\"AgentTask\")\n",
    "ObsType = TypeVar(\"ObsType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    track: bool = False\n",
    "    total_updates: int = 4000\n",
    "    rollout_length: int = 256\n",
    "    batch_size: int = 32\n",
    "    ent_coef: float = 0.0\n",
    "    vf_coef: float = 0.5\n",
    "    clip_coef: float = 0.2\n",
    "    learning_rate: float = 1e-4\n",
    "    epsilon: float = 1e-5\n",
    "    gamma: float = 0.995\n",
    "    gae_lambda: float = 0.95\n",
    "    epochs: int = 5\n",
    "    agent_curriculum: str = \"SP\"\n",
    "    env_curriculum: str = \"DR\"\n",
    "    agent_update_frequency: int = 8000\n",
    "    max_agents: int = 10\n",
    "    save_agent_checkpoints: bool = False\n",
    "    checkpoint_frequency: int = 4000\n",
    "    n_env_tasks: int = 4000\n",
    "    seed: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, possible_agents: np.ndarray):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {agent: x[idx] for idx, agent in enumerate(possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class LasertagParallelWrapper(TaskWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper ensuring compatibility with the PettingZoo Parallel API.\n",
    "\n",
    "    Lasertag Environment:\n",
    "        * Action shape:  `n_agents` * `Discrete(5)`\n",
    "        * Observation shape: Dict('image': Box(0, 255, (`n_agents`, 3, 5, 5), uint8))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_agents = n_agents\n",
    "        self.task = None\n",
    "        self.episode_return = 0\n",
    "        self.possible_agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Delegate attribute lookup to the wrapped environment if the attribute\n",
    "        is not found in the LasertagParallelWrapper instance.\n",
    "        \"\"\"\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def _np_array_to_pz_dict(self, array: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing individual observations for each agent.\n",
    "        Assumes that the batch dimension represents individual agents.\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        for idx, value in enumerate(array):\n",
    "            out[self.possible_agents[idx]] = value\n",
    "        return out\n",
    "\n",
    "    def _singleton_to_pz_dict(self, value: bool) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Broadcasts the `done` and `trunc` flags to dictionaries keyed by agent id.\n",
    "        \"\"\"\n",
    "        return {str(agent_index): value for agent_index in range(self.n_agents)}\n",
    "\n",
    "    def reset(\n",
    "        self, env_task: int\n",
    "    ) -> Tuple[Dict[AgentID, ObsType], Dict[AgentID, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment and returns a dictionary of observations\n",
    "        keyed by agent ID.\n",
    "        \"\"\"\n",
    "        self.env.seed(env_task)\n",
    "        obs = self.env.reset_random()  # random level generation\n",
    "        pz_obs = self._np_array_to_pz_dict(obs[\"image\"])\n",
    "\n",
    "        return pz_obs\n",
    "\n",
    "    def step(\n",
    "        self, action: Dict[AgentID, ActionType], device: str, agent_task: int\n",
    "    ) -> Tuple[\n",
    "        Dict[AgentID, ObsType],\n",
    "        Dict[AgentID, float],\n",
    "        Dict[AgentID, bool],\n",
    "        Dict[AgentID, bool],\n",
    "        Dict[AgentID, dict],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Takes inputs in the PettingZoo (PZ) Parallel API format, performs a step and\n",
    "        returns outputs in PZ format.\n",
    "        \"\"\"\n",
    "        action = batchify(action, device)\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        obs = obs[\"image\"]\n",
    "        trunc = False  # there is no `truncated` flag in this environment\n",
    "        self.task_completion = self._task_completion(obs, rew, done, trunc, info)\n",
    "        # convert outputs back to PZ format\n",
    "        obs, rew = map(self._np_array_to_pz_dict, [obs, rew])\n",
    "        done, trunc, info = map(\n",
    "            self._singleton_to_pz_dict, [done, trunc, self.task_completion]\n",
    "        )\n",
    "        info[\"agent_id\"] = agent_task\n",
    "        self.n_steps += 1\n",
    "\n",
    "        return self.observation(obs), rew, done, trunc, info\n",
    "\n",
    "\n",
    "# class Agent(nn.Module):\n",
    "#     def __init__(self, num_actions):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.network = nn.Sequential(\n",
    "#             self._layer_init(nn.Linear(3 * 5 * 5, 512)),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "#         self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "#     def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "#         torch.nn.init.orthogonal_(layer.weight, std)\n",
    "#         torch.nn.init.constant_(layer.bias, bias_const)\n",
    "#         return layer\n",
    "\n",
    "#     def get_value(self, x, flatten_start_dim=1):\n",
    "#         x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "#         return self.critic(self.network(x / 255.0))\n",
    "\n",
    "#     def get_action_and_value(self, x, action=None, flatten_start_dim=1):\n",
    "#         x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "#         hidden = self.network(x / 255.0)\n",
    "#         logits = self.actor(hidden)\n",
    "#         probs = Categorical(logits=logits)\n",
    "#         if action is None:\n",
    "#             action = probs.sample()\n",
    "#         return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution layer (3 Ã— 3 kernel, stride length 1, 16 filters)\n",
    "- flatten\n",
    "- Relu\n",
    "- LSTM (256)\n",
    "- Dense(32)\n",
    "- Relu\n",
    "- Dense(32)\n",
    "- Relu\n",
    "  => logits over 5 actions\n",
    "\n",
    "### TODO:\n",
    "\n",
    "**_NO AGENT DIRECTIONS AS INPUTS_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, input_channels: int, num_actions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            self.layer_init(\n",
    "                nn.Conv2d(input_channels, out_channels=16, kernel_size=3, stride=1)\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=16 * 3 * 3, hidden_size=256, batch_first=True)\n",
    "        self.lstm_init()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            self.layer_init(nn.Linear(256, 32)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # TODO: the paper doesn't mention the critic network?\n",
    "        self.actor = self.layer_init(nn.Linear(32, num_actions), scale=0.01)\n",
    "        self.critic = self.layer_init(nn.Linear(32, 1), scale=1)\n",
    "\n",
    "    def get_states(self, x, lstm_state, done):\n",
    "        # add batch dim if missing\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)  # shape: batch_size, *obs_shape\n",
    "\n",
    "        hidden = self.conv(x / 255.0)  # shape: batch_size, n_features, kernel, kernel\n",
    "\n",
    "        batch_size = hidden.size(0)\n",
    "        hidden = hidden.reshape(batch_size, -1)  # shape: batch_size, features\n",
    "        hidden = hidden.unsqueeze(1)  # add seq len dimension\n",
    "        # => shape: batch_size, seq_len=1, n_features\n",
    "\n",
    "        new_hidden = []\n",
    "        # reset lstm state if done\n",
    "        for h, d in zip(hidden, done):\n",
    "            h, lstm_state = self.lstm(\n",
    "                h.unsqueeze(0),\n",
    "                (\n",
    "                    torch.logical_not(d).view(1, -1, 1) * lstm_state[0],\n",
    "                    torch.logical_not(d).view(1, -1, 1) * lstm_state[1],\n",
    "                ),\n",
    "            )\n",
    "            new_hidden += [h]\n",
    "\n",
    "        new_hidden = torch.flatten(torch.cat(new_hidden), 0, 1)\n",
    "        return new_hidden, lstm_state\n",
    "\n",
    "    def get_value(self, x, lstm_state, done):\n",
    "        hidden, _ = self.get_states(x, lstm_state, done)\n",
    "        hidden = self.mlp(hidden)\n",
    "        return self.critic(hidden)\n",
    "\n",
    "    def get_action_and_value(self, x, lstm_state, done, action=None):\n",
    "        hidden, lstm_state = self.get_states(x, lstm_state, done)\n",
    "        hidden = self.mlp(hidden)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        return (\n",
    "            action,\n",
    "            probs.log_prob(action),\n",
    "            probs.entropy(),\n",
    "            self.critic(hidden),\n",
    "            lstm_state,\n",
    "        )\n",
    "\n",
    "    def layer_init(self, layer, scale=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, scale)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def lstm_init(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_curriculums = {\n",
    "    \"SP\": SelfPlay,\n",
    "    \"FSP\": FictitiousSelfPlay,\n",
    "    \"PFSP\": PrioritizedFictitiousSelfPlay,\n",
    "}\n",
    "env_curriculums = {\n",
    "    \"DR\": DomainRandomization,\n",
    "    \"PLR\": CentralizedPrioritizedLevelReplay,\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    \"\"\"ALGO PARAMS\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    stack_size = 3\n",
    "    frame_size = (5, 5)\n",
    "    n_agents = 2\n",
    "    num_actions = 5\n",
    "\n",
    "    agent_curriculum_settings = {\n",
    "        \"device\": device,\n",
    "        \"storage_path\": f\"{args.agent_curriculum}_agents\",\n",
    "        \"max_agents\": args.max_agents,\n",
    "        \"seed\": args.seed,\n",
    "    }\n",
    "\n",
    "    env_task_space = TaskSpace(spaces.Discrete(args.n_env_tasks))\n",
    "    env_curriculum_settings = {\n",
    "        \"DR\": {\"task_space\": env_task_space},\n",
    "        \"PLR\": {\n",
    "            \"task_space\": env_task_space,\n",
    "            \"num_steps\": args.rollout_length,\n",
    "            \"num_processes\": 1,  # TODO: modify if using vecenvs\n",
    "            \"gamma\": args.gamma,\n",
    "            \"gae_lambda\": args.gae_lambda,\n",
    "            \"task_sampler_kwargs_dict\": {\"strategy\": \"value_l1\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    \"\"\" LEARNER SETUP \"\"\"\n",
    "    agent = Agent(input_channels=stack_size, num_actions=num_actions).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=args.epsilon)\n",
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    env = LasertagAdversarial(record_video=False)  # 2 agents by default\n",
    "    env = LasertagParallelWrapper(env=env, n_agents=n_agents)\n",
    "    agent_curriculum = agent_curriculums[args.agent_curriculum](\n",
    "        agent=agent, **agent_curriculum_settings\n",
    "    )\n",
    "\n",
    "    env_curriculum = env_curriculums[args.env_curriculum](\n",
    "        **env_curriculum_settings[args.env_curriculum]\n",
    "    )\n",
    "\n",
    "    curriculum = DualCurriculumWrapper(\n",
    "        env=env,\n",
    "        agent_curriculum=agent_curriculum,\n",
    "        env_curriculum=env_curriculum,\n",
    "    )\n",
    "    mp_curriculum = make_multiprocessing_curriculum(curriculum)\n",
    "\n",
    "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    total_episodic_return = 0\n",
    "    rb_obs = torch.zeros((args.rollout_length, n_agents, stack_size, *frame_size)).to(\n",
    "        device\n",
    "    )\n",
    "    rb_actions = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "    rb_logprobs = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "    rb_terms = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "    rb_values = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "\n",
    "    agent_tasks, env_tasks, rewards_history = [], [], []\n",
    "    agent_c_rew, opp_c_rew = 0, 0\n",
    "    episode, n_learner_wins = 0, 0\n",
    "    n_updates = 0\n",
    "    info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones = {\n",
    "    \"agent_0\": 0,\n",
    "    \"agent_1\": 0,\n",
    "}\n",
    "lstm_state = (\n",
    "    torch.zeros(agent.lstm.num_layers, 1, agent.lstm.hidden_size).to(device),\n",
    "    torch.zeros(agent.lstm.num_layers, 1, agent.lstm.hidden_size).to(device),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ad54682de3407a87bb6602038a5ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtotal_updates) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m n_updates \u001b[38;5;241m<\u001b[39m args\u001b[38;5;241m.\u001b[39mtotal_updates:\n\u001b[1;32m----> 5\u001b[0m         initial_lstm_state \u001b[38;5;241m=\u001b[39m (\u001b[43mlstm_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, lstm_state[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mclone())\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      7\u001b[0m             env_task, agent_task \u001b[38;5;241m=\u001b[39m mp_curriculum\u001b[38;5;241m.\u001b[39msample()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "\n",
    "with tqdm(total=args.total_updates) as pbar:\n",
    "    while n_updates < args.total_updates:\n",
    "        initial_lstm_state = (lstm_state[0].clone(), lstm_state[1].clone())\n",
    "        with torch.no_grad():\n",
    "            env_task, agent_task = mp_curriculum.sample()\n",
    "\n",
    "            env_tasks.append(env_task)\n",
    "            agent_tasks.append(agent_task)\n",
    "\n",
    "            next_obs = env.reset(env_task)\n",
    "            total_episodic_return = 0\n",
    "\n",
    "            for step in range(0, args.rollout_length):\n",
    "                joint_obs = batchify(next_obs, device).squeeze()\n",
    "                agent_obs, opponent_obs = joint_obs\n",
    "\n",
    "                actions, logprobs, _, values, lstm_state = agent.get_action_and_value(\n",
    "                    agent_obs, lstm_state, batchify(dones, device)\n",
    "                )\n",
    "\n",
    "                opponent = mp_curriculum.get_opponent(info.get(\"agent_id\", 0)).to(\n",
    "                    device\n",
    "                )\n",
    "                opponent_action, *_ = opponent.get_action_and_value(\n",
    "                    opponent_obs, lstm_state, batchify(dones, device)\n",
    "                )\n",
    "\n",
    "                joint_actions = torch.tensor((actions, opponent_action))\n",
    "                next_obs, rewards, dones, truncs, info = env.step(\n",
    "                    unbatchify(joint_actions, env.possible_agents),\n",
    "                    device,\n",
    "                    agent_task,\n",
    "                )\n",
    "\n",
    "                opp_reward = rewards[\"agent_1\"]\n",
    "                if opp_reward != 0:\n",
    "                    mp_curriculum.update_winrate(info[\"agent_id\"], opp_reward)\n",
    "                    if opp_reward == -1:\n",
    "                        n_learner_wins += 1\n",
    "\n",
    "                rb_obs[step] = batchify(next_obs, device)\n",
    "                rb_rewards[step] = batchify(rewards, device)\n",
    "                rb_terms[step] = batchify(dones, device)\n",
    "                rb_actions[step] = joint_actions\n",
    "                rb_logprobs[step] = logprobs\n",
    "                rb_values[step] = values.flatten()\n",
    "\n",
    "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "                agent_c_rew += rewards[\"agent_0\"]\n",
    "                opp_c_rew += rewards[\"agent_1\"]\n",
    "                grid_size = env.level[3][\"grid_size_selected\"]\n",
    "                walls_percentage = env.level[3][\"clutter_rate_selected\"]\n",
    "\n",
    "                if any([dones[a] for a in dones]) or any([truncs[a] for a in truncs]):\n",
    "                    episode += 1\n",
    "                    rewards_history.append(rewards)\n",
    "                    env_task, agent_task = mp_curriculum.sample()\n",
    "                    env_tasks.append(env_task)\n",
    "                    agent_tasks.append(agent_task)\n",
    "\n",
    "                    next_obs = env.reset(env_task)\n",
    "\n",
    "            if args.env_curriculum == \"PLR\":\n",
    "                next_value = agent.get_value(\n",
    "                    torch.tensor(next_obs[\"agent_0\"]).to(device),\n",
    "                    flatten_start_dim=0,\n",
    "                )\n",
    "                update = {\n",
    "                    \"update_type\": \"on_demand\",\n",
    "                    \"metrics\": {\n",
    "                        \"value\": values,\n",
    "                        \"next_value\": next_value,\n",
    "                        \"rew\": rewards[\"agent_0\"],  # TODO: is this the expected use?\n",
    "                        \"dones\": dones,\n",
    "                        \"tasks\": env_task,\n",
    "                    },\n",
    "                }\n",
    "\n",
    "        # gae\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(\n",
    "                torch.tensor(next_obs[\"agent_0\"]).to(device), lstm_state, batchify(dones, device)\n",
    "            )\n",
    "            rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "            last_gae_lam = 0\n",
    "            for t in reversed(range(args.rollout_length - 1)):\n",
    "                if t == args.rollout_length - 1:\n",
    "                    next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                    next_values = next_value\n",
    "                else:\n",
    "                    next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                    next_values = rb_values[t + 1]\n",
    "                delta = (\n",
    "                    rb_rewards[t]\n",
    "                    + args.gamma * next_values * next_non_terminal\n",
    "                    - rb_values[t]\n",
    "                )\n",
    "                rb_advantages[t] = last_gae_lam = (\n",
    "                    delta\n",
    "                    + args.gamma * args.gae_lambda * next_non_terminal * last_gae_lam\n",
    "                )\n",
    "            rb_returns = rb_advantages + rb_values\n",
    "\n",
    "        b_obs = torch.flatten(rb_obs[: args.rollout_length], start_dim=0, end_dim=1)\n",
    "        b_logprobs = torch.flatten(\n",
    "            rb_logprobs[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_actions = torch.flatten(\n",
    "            rb_actions[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_returns = torch.flatten(\n",
    "            rb_returns[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_values = torch.flatten(\n",
    "            rb_values[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_advantages = torch.flatten(\n",
    "            rb_advantages[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_terms = torch.flatten(rb_terms[: args.rollout_length], start_dim=0, end_dim=1)\n",
    "\n",
    "        b_index = np.arange(len(b_obs))\n",
    "        clip_fracs = []\n",
    "        for repeat in range(args.epochs):\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_obs), args.batch_size):\n",
    "                # select the indices we want to train on\n",
    "                end = start + args.batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value, _ = agent.get_action_and_value(\n",
    "                    b_obs[batch_index],\n",
    "                    (\n",
    "                        initial_lstm_state[0][:, batch_index],\n",
    "                        initial_lstm_state[1][:, batch_index],\n",
    "                    ),\n",
    "                    b_terms[batch_index],\n",
    "                    b_actions.long()[batch_index],\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > args.clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                # normalize advantages\n",
    "                rb_advantages = b_advantages[batch_index]\n",
    "                rb_advantages = (rb_advantages - rb_advantages.mean()) / (\n",
    "                    rb_advantages.std() + 1e-8\n",
    "                )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                    ratio, 1 - args.clip_coef, 1 + args.clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                n_updates += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                # update opponent\n",
    "                if args.agent_curriculum in [\"FSP\", \"PFSP\"]:\n",
    "                    if n_updates % args.agent_update_frequency == 0 and episode != 0:\n",
    "                        mp_curriculum.update_agent(agent)\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
