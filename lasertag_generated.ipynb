{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import joblib\n",
    "from lasertag import LasertagAdversarial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from syllabus.core import (\n",
    "    Curriculum,\n",
    "    TaskWrapper,\n",
    "    PettingZooMultiProcessingSyncWrapper,\n",
    "    make_multiprocessing_curriculum,\n",
    ")\n",
    "from syllabus.task_space import TaskSpace\n",
    "from syllabus.curricula import DomainRandomization\n",
    "\n",
    "ObsType = TypeVar(\"ObsType\")\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "AgentID = TypeVar(\"AgentID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, possible_agents:np.ndarray):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(possible_agents)}\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LasertagParallelWrapper(TaskWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper ensuring compatibility with the PettingZoo Parallel API.\n",
    "\n",
    "    Lasertag Environment:\n",
    "        * Action shape:  `n_agents` * `Discrete(5)`\n",
    "        * Observation shape: Dict('image': Box(0, 255, (`n_agents`, 3, 5, 5), uint8))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_agents = n_agents\n",
    "        self.task = None\n",
    "        self.episode_return = 0\n",
    "        self.task_space = TaskSpace(spaces.MultiDiscrete(np.array([[2], [5]])))\n",
    "        self.possible_agents = np.arange(self.n_agents)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Delegate attribute lookup to the wrapped environment if the attribute\n",
    "        is not found in the LasertagParallelWrapper instance.\n",
    "        \"\"\"\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def _np_array_to_pz_dict(self, array: np.ndarray) -> dict[str : np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing individual observations for each agent.\n",
    "        Assumes that the batch dimension represents individual agents.\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        for idx, i in enumerate(array):\n",
    "            out[str(idx)] = i\n",
    "        return out\n",
    "\n",
    "    def _singleton_to_pz_dict(self, value: bool) -> dict[str:bool]:\n",
    "        \"\"\"\n",
    "        Broadcasts the `done` and `trunc` flags to dictionaries keyed by agent id.\n",
    "        \"\"\"\n",
    "        return {str(idx): value for idx in range(self.n_agents)}\n",
    "\n",
    "    def reset(self) -> tuple[dict[AgentID, ObsType], dict[AgentID, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment and returns a dictionary of observations\n",
    "        keyed by agent ID.\n",
    "        \"\"\"\n",
    "        obs = self.env.reset()\n",
    "        pz_obs = self._np_array_to_pz_dict(obs[\"image\"])\n",
    "\n",
    "        return pz_obs\n",
    "\n",
    "    def step(self, action: dict[AgentID, ActionType], device: str) -> tuple[\n",
    "        dict[AgentID, ObsType],\n",
    "        dict[AgentID, float],\n",
    "        dict[AgentID, bool],\n",
    "        dict[AgentID, bool],\n",
    "        dict[AgentID, dict],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Takes inputs in the PettingZoo (PZ) Parallel API format, performs a step and\n",
    "        returns outputs in PZ format.\n",
    "        \"\"\"\n",
    "        action = batchify(action, device)\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        obs = obs[\"image\"]\n",
    "        trunc = 0  # there is no `truncated` flag in this environment\n",
    "        self.task_completion = self._task_completion(obs, rew, done, trunc, info)\n",
    "        # convert outputs back to PZ format\n",
    "        obs, rew = tuple(map(self._np_array_to_pz_dict, [obs, rew]))\n",
    "        done, trunc, info = tuple(\n",
    "            map(self._singleton_to_pz_dict, [done, trunc, self.task_completion])\n",
    "        )\n",
    "\n",
    "        return self.observation(obs), rew, done, trunc, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlay(Curriculum):\n",
    "    def __init__(self, agent, device: str, store_agents_on_cpu: bool = False):\n",
    "        self.store_agents_on_cpu = store_agents_on_cpu\n",
    "        self.storage_device = \"cpu\" if self.store_agents_on_cpu else device\n",
    "        self.agent = deepcopy(agent).to(self.storage_device)\n",
    "\n",
    "    def update_agent(self, agent):\n",
    "        self.agent = deepcopy(agent).to(self.storage_device)\n",
    "\n",
    "    def get_opponent(self, agent_id):\n",
    "        assert (\n",
    "            agent_id == 0\n",
    "        ), f\"Self play only tracks the current agent. Expected agent id 0, got {agent_id}\"\n",
    "        return self.agent, 0\n",
    "\n",
    "    def sample(self, k=1):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Linear(3 * 5 * 5, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ALGO PARAMS\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.1\n",
    "vf_coef = 0.1\n",
    "clip_coef = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "stack_size = 3\n",
    "frame_size = (5, 5)\n",
    "max_cycles = 201 # lasertag has 200 maximum steps by default\n",
    "total_episodes = 500\n",
    "n_agents = 2\n",
    "num_actions = 5\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "agent = Agent(num_actions=num_actions).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "\"\"\" ENV SETUP \"\"\"\n",
    "env = LasertagAdversarial(record_video=False) # 2 agents by default\n",
    "env = LasertagParallelWrapper(env=env, n_agents=n_agents)\n",
    "curriculum = SelfPlay(agent=agent, device=device, store_agents_on_cpu=True)\n",
    "observation_size = env.observation_space[\"image\"].shape[1:]\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, n_agents, stack_size, *frame_size)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "\n",
    "losses, episode_rewards = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=75, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (actor): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (critic): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curriculum.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_action_and_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m actions, logprobs, _, values \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action_and_value(\n\u001b[0;32m     20\u001b[0m     agent_obs, flatten_start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m opponent \u001b[38;5;241m=\u001b[39m curriculum\u001b[38;5;241m.\u001b[39mget_opponent(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m opponent_action, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[43mopponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_value\u001b[49m(\n\u001b[0;32m     25\u001b[0m     opponent_obs, flatten_start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# execute the environment and log data\u001b[39;00m\n\u001b[0;32m     28\u001b[0m joint_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor((actions, opponent_action))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_action_and_value'"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "\n",
    "# train for n number of episodes\n",
    "for episode in tqdm(range(total_episodes)):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        next_obs = env.reset_random()  # removed seed=None and info\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            # rollover the observation\n",
    "            joint_obs = batchify(next_obs, device).squeeze()\n",
    "            agent_obs, opponent_obs = joint_obs\n",
    "\n",
    "            # get action from the agent and the opponent\n",
    "            actions, logprobs, _, values = agent.get_action_and_value(\n",
    "                agent_obs, flatten_start_dim=0\n",
    "            )\n",
    "\n",
    "            opponent = curriculum.get_opponent(0)\n",
    "            opponent_action, *_ = opponent.get_action_and_value(\n",
    "                opponent_obs, flatten_start_dim=0\n",
    "            )\n",
    "            # execute the environment and log data\n",
    "            joint_actions = torch.tensor((actions, opponent_action))\n",
    "            next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                unbatchify(joint_actions, env.possible_agents), device\n",
    "            )\n",
    "            # next_obs, rewards, terms, truncs, infos = env.step(joint_actions)\n",
    "            episode_rewards.append(rewards)\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = batchify(next_obs, device)\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = joint_actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values.flatten()\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "        for t in reversed(range(end_step)):\n",
    "            delta = (\n",
    "                rb_rewards[t]\n",
    "                + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
    "                - rb_values[t]\n",
    "            )\n",
    "            rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
    "        rb_returns = rb_advantages + rb_values\n",
    "    # convert our episodes to batch of individual transitions\n",
    "    b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "    b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "    b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "    b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_index = np.arange(len(b_obs))\n",
    "    clip_fracs = []\n",
    "    for repeat in range(3):\n",
    "        # shuffle the indices we use to access the data\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_obs), batch_size):\n",
    "            # select the indices we want to train on\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                b_obs[batch_index], b_actions.long()[batch_index]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            # normalize advantages\n",
    "            advantages = b_advantages[batch_index]\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "            pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                ratio, 1 - clip_coef, 1 + clip_coef\n",
    "            )\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "            losses.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # update opponent\n",
    "    curriculum.update_agent(agent)\n",
    "\n",
    "print(f\"Training episode {episode}\")\n",
    "print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "print(f\"Episode Length: {end_step}\")\n",
    "print(\"\")\n",
    "print(f\"Value Loss: {v_loss.item()}\")\n",
    "print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "print(f\"Approx KL: {approx_kl.item()}\")\n",
    "print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "print(f\"Explained Variance: {explained_var.item()}\")\n",
    "print(\"\\n-------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
